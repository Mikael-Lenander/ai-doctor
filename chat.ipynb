{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 13:03:57.347258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735643037.366618   16596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735643037.372661   16596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-31 13:03:59,725] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikael/anaconda3/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"\n",
    "EPOCHS = 2\n",
    "DEEPSPEED_CONFIG = \"deepspeed_config.json\"\n",
    "DEVICE.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    ")#.to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\", truncation=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User: I’ve been sneezing a lot today and my no...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User: I’ve developed a rash after eating some ...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User: My eyes are swollen and itchy, and I can...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User: I’ve been getting headaches and a stuffy...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User: Every time I eat nuts, my mouth itches. ...</td>\n",
       "      <td>allergy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  User: I’ve been sneezing a lot today and my no...  allergy\n",
       "1  User: I’ve developed a rash after eating some ...  allergy\n",
       "2  User: My eyes are swollen and itchy, and I can...  allergy\n",
       "3  User: I’ve been getting headaches and a stuffy...  allergy\n",
       "4  User: Every time I eat nuts, my mouth itches. ...  allergy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(kagglehub.dataset_download(\"artemminiailo/medicalconversations2disease\")) / \"medical_conversations.csv\"\n",
    "df = pd.read_csv(path, names=('text', 'label'), skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate chat templates from raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_template(conversation):\n",
    "    roles = {\"user\": \"user\", \"bot\": \"assistant\"}\n",
    "    lines = conversation.split(\"</s>\")\n",
    "    template = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an empathetic doctor chatbot that asks follow-up questions about the patient's symptoms and explains very briefly what the health problem might be.\",\n",
    "        }\n",
    "    ]\n",
    "    for line in lines:\n",
    "        sep_idx = line.find(\":\")\n",
    "        if sep_idx == -1:\n",
    "            continue\n",
    "        message = {\n",
    "            \"role\": roles[line[:sep_idx].lower().strip()],\n",
    "            \"content\": line[sep_idx + 1:].strip(),\n",
    "        }\n",
    "        template.append(message)\n",
    "    templates = []\n",
    "    for i, message in enumerate(template):\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            templates.append(template[:i+1])\n",
    "    return templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mask(input_ids):\n",
    "    pad_length = len(list(itertools.takewhile(lambda x: x == tokenizer.pad_token_id, input_ids)))\n",
    "    return torch.cat([torch.zeros(pad_length), torch.ones(len(input_ids) - pad_length)]).long()\n",
    "\n",
    "def generate_training_data(chat_templates):\n",
    "    inputs_tokenized = tokenizer.apply_chat_template(chat_templates, padding=True, return_tensors=\"pt\")\n",
    "    inputs_left_shifted = inputs_tokenized[:, :-1]\n",
    "\n",
    "    attention_mask = torch.stack([pad_mask(input_ids) for input_ids in inputs_left_shifted])\n",
    "\n",
    "    label_templates = [[template[-1]] for template in chat_templates]\n",
    "    labels_tokenized = tokenizer.apply_chat_template(\n",
    "        label_templates,\n",
    "        padding=\"max_length\",\n",
    "        max_length=inputs_tokenized.shape[1],\n",
    "        # add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels_fixed = torch.where(torch.stack([pad_mask(labels) for labels in labels_tokenized]) == 1, labels_tokenized, -100)\n",
    "    labels_right_shifted = labels_fixed[:, 1:]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input_ids': inputs_left_shifted.cpu(),\n",
    "        'attention_mask': attention_mask.cpu(),\n",
    "        'labels': labels_right_shifted.cpu(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 106]) torch.Size([2, 106]) torch.Size([2, 106])\n",
      "input_ids [\"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>You are an empathetic doctor chatbot that asks follow-up questions about the patient's symptoms and explains very briefly what the health problem might be.<|endoftext|>I’ve been sneezing a lot today and my nose feels congested.<|endoftext|>That sounds like it could be an allergy. Do you know what might be triggering it?\", \"You are an empathetic doctor chatbot that asks follow-up questions about the patient's symptoms and explains very briefly what the health problem might be.<|endoftext|>I’ve been sneezing a lot today and my nose feels congested.<|endoftext|>That sounds like it could be an allergy. Do you know what might be triggering it?<|endoftext|>I’m not sure. Maybe pollen?<|endoftext|>Pollen is a common allergen. Have you had any other symptoms, like itchy eyes or a sore throat?\"]\n",
      "mask tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "ids tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256,  1639,   389,   281,\n",
      "           795,  8071,  6587,  6253,  8537, 13645,   326,  7893,  1061,    12,\n",
      "           929,  2683,   546,   262,  5827,   338,  7460,   290,  6688,   845,\n",
      "         11589,   644,   262,  1535,  1917,  1244,   307,    13, 50256,    40,\n",
      "           447,   247,   303,   587, 10505,  8471,   278,   257,  1256,  1909,\n",
      "           290,   616,  9686,  5300, 22791,   276,    13, 50256,  2504,  5238,\n",
      "           588,   340,   714,   307,   281, 36197,    13,  2141,   345,   760,\n",
      "           644,  1244,   307, 26555,   340,    30],\n",
      "        [ 1639,   389,   281,   795,  8071,  6587,  6253,  8537, 13645,   326,\n",
      "          7893,  1061,    12,   929,  2683,   546,   262,  5827,   338,  7460,\n",
      "           290,  6688,   845, 11589,   644,   262,  1535,  1917,  1244,   307,\n",
      "            13, 50256,    40,   447,   247,   303,   587, 10505,  8471,   278,\n",
      "           257,  1256,  1909,   290,   616,  9686,  5300, 22791,   276,    13,\n",
      "         50256,  2504,  5238,   588,   340,   714,   307,   281, 36197,    13,\n",
      "          2141,   345,   760,   644,  1244,   307, 26555,   340,    30, 50256,\n",
      "            40,   447,   247,    76,   407,  1654,    13,  6674, 40362,    30,\n",
      "         50256,    47, 29952,   318,   257,  2219,   477,   263,  5235,    13,\n",
      "          8192,   345,   550,   597,   584,  7460,    11,   588,   340, 29658,\n",
      "          2951,   393,   257, 19597, 13589,    30]])\n",
      "lbl tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2504,  5238,   588,\n",
      "           340,   714,   307,   281, 36197,    13,  2141,   345,   760,   644,\n",
      "          1244,   307, 26555,   340,    30, 50256],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "            47, 29952,   318,   257,  2219,   477,   263,  5235,    13,  8192,\n",
      "           345,   550,   597,   584,  7460,    11,   588,   340, 29658,  2951,\n",
      "           393,   257, 19597, 13589,    30, 50256]])\n"
     ]
    }
   ],
   "source": [
    "chat_templates = list(itertools.chain(*map(generate_template, df['text'])))[:5] # chain flattens 2d list to 1d\n",
    "\n",
    "template = chat_templates[:2]\n",
    "res = generate_training_data(chat_templates[:2])\n",
    "print(res['input_ids'].shape, res['attention_mask'].shape, res['labels'].shape)\n",
    "print('input_ids', tokenizer.batch_decode(res['input_ids']))\n",
    "# print('labels', [tokenizer.batch_decode(res['labels'][i, :]) for i in range(2)])\n",
    "print('mask', res['attention_mask'])\n",
    "print('ids', res['input_ids'])\n",
    "print('lbl', res['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    return loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversations(Dataset):\n",
    "    def __init__(self, df):\n",
    "        chat_templates = list(itertools.chain(*map(generate_template, df['text']))) # chain flattens 2d list to 1d\n",
    "        self.sequences = generate_training_data(chat_templates)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.sequences['input_ids'][idx], self.sequences['attention_mask'][idx]), self.sequences['labels'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = training_data['input_ids'][0].unsqueeze(0).to(DEVICE)\n",
    "# mask = training_data['attention_mask'][0].unsqueeze(0).to(DEVICE)\n",
    "# labels = training_data['labels'][0].to(DEVICE)\n",
    "# logits = model(input_ids, attention_mask=mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,622,016 || all params: 126,061,824 || trainable%: 1.2867\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['c_attn', 'c_proj']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Conversations(df)\n",
    "loader = DataLoader(dataset, batch_size=8)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "# model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "#     model=model,\n",
    "#     model_parameters=model.parameters(),\n",
    "#     config_params=DEEPSPEED_CONFIG,\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n",
      "check\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# model_engine.backward(loss)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# model_engine.step()\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     20\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/cuda/memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 192\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, ((input_ids, attention_masks), labels) in enumerate(loader):\n",
    "        print(f'Batch {i+1}/{len(loader)}', end='\\r')\n",
    "        print()\n",
    "        # outputs = model_engine(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        outputs = model(input_ids=input_ids.to(DEVICE), attention_mask=attention_masks.to(DEVICE))\n",
    "        loss = calculate_loss(outputs.logits, labels.to(DEVICE))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model_engine.backward(loss)\n",
    "        # model_engine.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    train_loss = total_loss / len(loader)\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    chat_templates[0][:-1],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    padding_side='left',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are an empathetic doctor chatbot that asks follow-up questions about the patient's symptoms and explains very briefly what the health problem might be.<|im_end|>\\n<|im_start|>user\\nI’ve been sneezing a lot today and my nose feels congested.<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(t2.size(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
